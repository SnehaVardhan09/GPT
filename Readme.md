Building GPT


tokenization:
    current project- Character level tokenizer
    google-SentencePiece
    openAI-Tiktoken [50257 vocabsize]

we train transformer with chunks of data,we have to decide the context lenght (block size)
